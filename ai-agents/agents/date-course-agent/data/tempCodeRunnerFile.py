# ì‹¤ì œ ì¥ì†Œ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# - data/places ë””ë ‰í† ë¦¬ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì„ë² ë”© ìƒì„± í›„ Qdrantì— ì €ì¥

import asyncio
import json
import os
import sys
from typing import List, Dict, Any
from loguru import logger
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.core.embedding_service import EmbeddingService
from src.database.qdrant_client import get_qdrant_client
from config.settings import Settings

class VectorDBLoader:
    """data/places ë””ë ‰í† ë¦¬ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ë¡œë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedding_service = None
        self.qdrant_client = get_qdrant_client()
        # í”„ë¡œì íŠ¸ ë‚´ë¶€ places ë””ë ‰í† ë¦¬ ì‚¬ìš©
        self.places_data_path = os.path.join(os.path.dirname(__file__), "places")
        self.batch_size = 20  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (ë„ˆë¬´ í¬ë©´ API í•œë„ ì´ˆê³¼)
        
        # ì¹´í…Œê³ ë¦¬ íŒŒì¼ ëª©ë¡
        self.category_files = [
            "ìŒì‹ì .json", "ì¹´í˜.json", "ìˆ ì§‘.json", "ë¬¸í™”ì‹œì„¤.json", 
            "íœ´ì‹ì‹œì„¤.json", "ì•¼ì™¸í™œë™.json", "ì—”í„°í…Œì¸ë¨¼íŠ¸.json", 
            "ì‡¼í•‘.json", "ì£¼ì°¨ì¥.json", "ê¸°íƒ€.json"
        ]
        
        logger.info(f"âœ… ë²¡í„° DB ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ - ë°ì´í„° ê²½ë¡œ: {self.places_data_path}")
    
    async def load_all_data(self):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ë¡œë“œ"""
        try:
            logger.info("ğŸš€ ë²¡í„° DB ë¡œë”© ì‹œì‘")
            start_time = time.time()
            
            # places ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
            if not os.path.exists(self.places_data_path):
                raise FileNotFoundError(f"âŒ places ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.places_data_path}")
            
            # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            self.embedding_service = EmbeddingService()
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
            logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”")
            self.qdrant_client.clear_collection()
            
            total_loaded = 0
            successful_categories = 0
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì²˜ë¦¬
            for category_file in self.category_files:
                category_name = category_file.replace('.json', '')
                logger.info(f"ğŸ“‚ {category_name} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘")
                
                loaded_count = await self.load_category_data(category_file, category_name)
                if loaded_count > 0:
                    successful_categories += 1
                    total_loaded += loaded_count
                    logger.info(f"âœ… {category_name} ì™„ë£Œ - {loaded_count}ê°œ ë¡œë“œ")
                else:
                    logger.warning(f"âš ï¸ {category_name} - ë¡œë“œëœ ë°ì´í„° ì—†ìŒ")
                
                # ì ì‹œ ëŒ€ê¸° (API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€)
                await asyncio.sleep(1)
            
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"ğŸ‰ ì „ì²´ ë¡œë”© ì™„ë£Œ!")
            logger.info(f"   ì„±ê³µí•œ ì¹´í…Œê³ ë¦¬: {successful_categories}/{len(self.category_files)}")
            logger.info(f"   ì´ {total_loaded}ê°œ ì¥ì†Œ ë¡œë“œ")
            logger.info(f"   ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
            
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = self.qdrant_client.get_collection_info()
            logger.info(f"ğŸ“Š ìµœì¢… ì»¬ë ‰ì…˜ ì •ë³´: {collection_info}")
            
            if total_loaded == 0:
                logger.error("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                logger.error("   ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                logger.error(f"   1. íŒŒì¼ ê²½ë¡œ: {self.places_data_path}")
                logger.error("   2. JSON íŒŒì¼ë“¤ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€")
                logger.error("   3. íŒŒì¼ ë‚´ìš©ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€")
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def load_category_data(self, category_file: str, category_name: str) -> int:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ"""
        try:
            file_path = os.path.join(self.places_data_path, category_file)
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                return 0
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(file_path)
            logger.info(f"ğŸ“„ {category_file} - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            
            # JSON íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            logger.info(f"ğŸ“– {category_file} ì½ê¸° ì™„ë£Œ - {len(raw_data)}ê°œ í•­ëª©")
            
            # ë°ì´í„° ê²€ì¦ ë° í•„í„°ë§
            valid_data = self.validate_and_filter_data(raw_data, category_name)
            logger.info(f"âœ… ìœ íš¨í•œ ë°ì´í„° - {len(valid_data)}ê°œ í•­ëª©")
            
            if not valid_data:
                logger.warning(f"âš ï¸ {category_name}ì— ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")
                return 0
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            total_processed = 0
            batch_count = (len(valid_data) - 1) // self.batch_size + 1
            
            for i in range(0, len(valid_data), self.batch_size):
                batch = valid_data[i:i + self.batch_size]
                batch_num = i // self.batch_size + 1
                
                logger.info(f"ğŸ”„ {category_name} ë°°ì¹˜ {batch_num}/{batch_count} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
                
                processed_count = await self.process_batch(batch, category_name)
                total_processed += processed_count
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (batch_num / batch_count) * 100
                logger.info(f"   ğŸ“ˆ {category_name} ì§„í–‰ë¥ : {progress:.1f}% ({total_processed}/{len(valid_data)})")
                
                # ë°°ì¹˜ ê°„ ëŒ€ê¸° (API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€)
                if i + self.batch_size < len(valid_data):
                    await asyncio.sleep(2)
            
            return total_processed
            
        except Exception as e:
            logger.error(f"âŒ {category_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return 0
    
    def validate_and_filter_data(self, raw_data: List[Dict], category_name: str) -> List[Dict]:
        """ë°ì´í„° ê²€ì¦ ë° í•„í„°ë§"""
        valid_data = []
        skipped_count = 0
        
        for item in raw_data:
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ìƒˆë¡œìš´ JSON êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                if not all(key in item for key in ['place_id', 'name', 'latitude', 'longitude', 'summary']):
                    skipped_count += 1
                    continue
                
                # place_idê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                if not item['place_id'] or len(item['place_id'].strip()) < 5:
                    skipped_count += 1
                    continue
                
                # summaryê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                if not item['summary'] or len(item['summary'].strip()) < 10:
                    skipped_count += 1
                    continue
                
                # nameì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                if not item['name'] or len(item['name'].strip()) < 1:
                    skipped_count += 1
                    continue
                
                # ìœ„ë„ ê²½ë„ ìœ íš¨ì„± í™•ì¸
                try:
                    lat = float(item['latitude'])
                    lon = float(item['longitude'])
                    if not (33 <= lat <= 39 and 124 <= lon <= 132):  # ëŒ€í•œë¯¼êµ­ ë²”ìœ„
                        skipped_count += 1
                        continue
                except (ValueError, TypeError):
                    skipped_count += 1
                    continue
                
                # ìœ íš¨í•œ ë°ì´í„°ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ JSON êµ¬ì¡° ì§€ì›)
                valid_item = {
                    'place_id': item['place_id'].strip(),  # ì‹¤ì œ place_id ì‚¬ìš©
                    'place_name': item['name'].strip(),
                    'latitude': float(item['latitude']),
                    'longitude': float(item['longitude']),
                    'address': item.get('address', '').strip(),  # ìƒˆë¡œ ì¶”ê°€
                    'kakao_url': item.get('kakao_url', '').strip(),  # ìƒˆë¡œ ì¶”ê°€
                    'description': item.get('description', '').strip(),
                    'summary': item['summary'].strip(),
                    'category': category_name,
                    'price': item.get('price', [])
                    # 'original_id': item.get('id', 0)  # ê¸°ì¡´ idëŠ” optional
                }
                
                valid_data.append(valid_item)
                
            except Exception as e:
                skipped_count += 1
                logger.debug(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
                continue
        
        if skipped_count > 0:
            logger.info(f"   âš ï¸ ìŠ¤í‚µëœ í•­ëª©: {skipped_count}ê°œ")
        
        return valid_data
    
    async def process_batch(self, batch: List[Dict], category_name: str) -> int:
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (ì„ë² ë”© ìƒì„± ë° ì €ì¥)"""
        try:
            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ì¤€ë¹„ (description + summary)
            embedding_texts = []
            for item in batch:
                # descriptionê³¼ summary ê²°í•©
                combined_text = f"{item['description']} {item['summary']}".strip()
                if not combined_text or combined_text == item['summary']:
                    combined_text = item['summary']  # descriptionì´ ë¹„ì–´ìˆìœ¼ë©´ summaryë§Œ ì‚¬ìš©
                embedding_texts.append(combined_text)
            
            # ì„ë² ë”© ìƒì„±
            logger.debug(f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... ({len(embedding_texts)}ê°œ)")
            embeddings = await self.embedding_service.create_embeddings(embedding_texts)
            
            # ë²¡í„° DBì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„ (ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€)
            places_data = []
            for item, embedding in zip(batch, embeddings):
                place_data = {
                    'place_id': item['place_id'],  # ì‹¤ì œ place_id ì‚¬ìš©
                    'place_name': item['place_name'],
                    'latitude': item['latitude'],
                    'longitude': item['longitude'],
                    'address': item['address'],  # ìƒˆë¡œ ì¶”ê°€
                    'kakao_url': item['kakao_url'],  # ìƒˆë¡œ ì¶”ê°€
                    'description': f"{item['description']} {item['summary']}".strip(),  # ë²¡í„° ìƒì„±ìš©
                    'summary': item['summary'],  # ì›ë³¸ summary ë³´ê´€
                    'category': item['category'],
                    'embedding_vector': embedding,
                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                    'price': item['price']
                    # 'original_id': item['original_id']
                }
                places_data.append(place_data)
            
            # Qdrantì— ì €ì¥
            logger.debug(f"ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘... ({len(places_data)}ê°œ)")
            self.qdrant_client.add_places(places_data)
            
            logger.debug(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - {len(places_data)}ê°œ ì €ì¥")
            return len(places_data)
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return 0
    
    async def test_search(self):
        """ë¡œë”© í›„ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            logger.info("ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
            test_queries = [
                "ë¡œë§¨í‹±í•œ ë ˆìŠ¤í† ë‘ì—ì„œ ì €ë… ì‹ì‚¬",
                "ì»¤í”¼ì™€ ë””ì €íŠ¸ë¥¼ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì¹´í˜",
                "ë¬¸í™” í™œë™ì„ í•  ìˆ˜ ìˆëŠ” ì¥ì†Œ"
            ]
            
            for query in test_queries:
                logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
                
                test_embedding = await self.embedding_service.create_single_embedding(query)
                
                # ê²€ìƒ‰ ì‹¤í–‰
                results = await self.qdrant_client.search_vectors(
                    query_vector=test_embedding,
                    limit=3
                )
                
                logger.info(f"   ğŸ“‹ ê²°ê³¼ {len(results)}ê°œ:")
                for i, result in enumerate(results):
                    logger.info(f"      {i+1}. {result['place_name']} ({result['category']}) - ìœ ì‚¬ë„: {result['similarity_score']:.3f}")
                logger.info("")
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    def show_file_status(self):
        """íŒŒì¼ ìƒíƒœ í™•ì¸"""
        logger.info("ğŸ“‹ íŒŒì¼ ìƒíƒœ í™•ì¸:")
        logger.info(f"   ğŸ“ ë°ì´í„° ê²½ë¡œ: {self.places_data_path}")
        
        for category_file in self.category_files:
            file_path = os.path.join(self.places_data_path, category_file)
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                logger.info(f"   âœ… {category_file} - {file_size:,} bytes")
            else:
                logger.info(f"   âŒ {category_file} - íŒŒì¼ ì—†ìŒ")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        loader = VectorDBLoader()
        
        # íŒŒì¼ ìƒíƒœ í™•ì¸
        loader.show_file_status()
        
        # ëª¨ë“  ë°ì´í„° ë¡œë“œ
        await loader.load_all_data()
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        await loader.test_search()
        
        logger.info("ğŸ‰ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
        logger.info("ì´ì œ 'python src/main.py'ë¡œ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    # ë¡œê±° ì„¤ì •
    logger.add("vector_db_loading.log", rotation="1 day", level="INFO")
    
    # ì‹¤í–‰
    asyncio.run(main())
